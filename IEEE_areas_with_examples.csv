Area,Topic,Type,Title,Text,Link1,Area 1,Description,,"This area focuses on theoretical and empirical research topics that aim to establish the foundation of VIS as a scientific subject. Theoretical work which aims to contribute to fundamental questions that relate to how we understand, assess, categorize, or formalize visualizations and/or visual analytics work. Empirical research aims to contribute research methodologies or concrete results of assessments of a visualization / visual analytics contribution or its context of use.",1,Concept Formulation,Description,,"surveys with organization, synthesis, and reflection; taxonomies and ontologies; guidelines and principles; lexica, syntaxes (grammars), semantics, pragmatics of visualization; and information security, privacy, ethics and professionalism in visualization.",1,Model Development,Description,,"conceptual models and simulation models for describing aspects of visualization processes (e.g., color perception, knowledge acquisition, collaborative decision making, etc.).",1,Mathematical Formalization,Description,,"mathematical frameworks, quality metrics, theorems (i.e., mathematically-defined causal relations in VIS).",1,Research Methodology,Description,,"general methodologies for conducting VIS research, e.g., typology, grounded theory, empirical studies, design studies, task analysis, user engagement, qualitative and quantitative research, etc.",1,Empirical Studies,Description,,"controlled (e.g., typical laboratory experiments), semi-controlled (e.g., typical crowdsourcing studies), and uncontrolled studies (e.g., small group discussions, think aloud exercises, field observation, ethnographic studies, etc.), which may be in the forms of qualitative or quantitative research and which may be further categorized according to their objectives as follows: Empirical Studies for Evaluation: studies for assessing the effectiveness and usability of specific VIS techniques, tools, systems, and workflows, for collecting lessons learned from failures, and for establishing the best practice. Empirical Studies for Observation, Data Acquisition, and Hypothesis Formulation: studies for observing phenomena in visualization processes, stimulating hypothesis formulation, and collecting data to inform computational models and quality metrics. Empirical Studies for Understanding and Theory Validation: studies for understanding the human factors in visualization processes, including perceptual factors (e.g., visual and nonvisual sensory processes, perception, attention, etc.) and cognitive factors (e.g., memory, learning, reasoning, decision-making, problem-solving, knowledge, emotion, etc.)",1,Concept Formulation,Paper,What Do We Talk About When We Talk About Dashboards?,"Dashboards are one of the most common use cases for data visualization, and their design and contexts of use areconsiderably different from exploratory visualization tools. In this paper, we look at the broad scope of how dashboards are usedin practice through an analysis of dashboard examples and documentation about their use. We systematically review the literaturesurrounding dashboard use, construct a design space for dashboards, and identify major dashboard types. We characterize dashboardsby their design goals, levels of interaction, and the practices around them. Our framework and literature review suggest a number offruitful research directions to better support dashboard design, implementation, and use.",https://alper.datav.is/assets/publications/dashboards/dashboards-preprint.pdf1,Model Development,Paper,A Model of Spatial Directness in Interactive Visualization.,"We discuss the concept of directness in the context of spatial interaction with visualization. In particular, we propose amodel that allows practitioners to analyze and describe the spatial directness of interaction techniques, ultimately to be able to betterunderstand interaction issues that may affect usability. To reach these goals, we distinguish between different types of directness.Each type of directness depends on a particular mapping between different spaces, for which we consider the data space, thevisualization space, the output space, the user space, the manipulation space, and the interaction space. In addition to the introductionof the model itself, we also show how to apply it to several real-world interaction scenarios in visualization, and thus discuss theresulting types of spatial directness, without recommending either more direct or more indirect interaction techniques. In particular, wewill demonstrate descriptive and evaluative usage of the proposed model, and also briefly discuss its generative usage.",https://ieeexplore-ieee-org.myaccess.library.utoronto.ca/stamp/stamp.jsp?tp=&arnumber=83950681,Mathematical Formalization,Paper,An Algebraic Process for Visualization Design,"We present a model of visualization design based on algebraic considerations of the visualization process. The modelhelps characterize visual encodings, guide their design, evaluate their effectiveness, and highlight their shortcomings. The model hasthree components: the underlying mathematical structure of the data or object being visualized, the concrete representation of the datain a computer, and (to the extent possible) a mathematical description of how humans perceive the visualization. Because we believethe value of our model lies in its practical application, we propose three general principles for good visualization design. We workthrough a collection of examples where our model helps explain the known properties of existing visualizations methods, both goodand not-so-good, as well as suggesting some novel methods. We describe how to use the model alongside experimental user studies,since it can help frame experiment outcomes in an actionable manner. Exploring the implications and applications of our model and itsdesign principles should provide many directions for future visualization research.",http://vis.cs.ucdavis.edu/vis2014papers/TVCG/papers/2181_20tvcg12-kindlmann-2346325.pdf1,Research Methodology,Paper,The elicitation interview technique: capturing people’s experiences of data representations,"Information visualization has become a popular tool to facilitate sense-making, discovery and communication in a large range of professional and casual contexts. However, evaluating visualizations is still a challenge. In particular, we lack techniques to help understand how visualizations are experienced by people. In this paper we discuss the potential of the Elicitation Interview technique to be applied in the context of visualization. The Elicitation Interview is a method for gathering detailed and precise accounts of human experience. We argue that it can be applied to help understand how people experience and interpret visualizations as part of exploration and data analysis processes. We describe the key characteristics of this interview technique and present a study we conducted to exemplify how it can be applied to evaluate data representations. Our study illustrates the types of insights this technique can bring to the fore, for example, evidence for deep interpretation of visual representations and the formation of interpretations and stories beyond the represented data. We discuss general visualization evaluation scenarios where the Elicitation Interview technique may be beneficial and specify what needs to be considered when applying this technique in a visualization context specifically.",https://ieeexplore-ieee-org.myaccess.library.utoronto.ca/document/73699911,Empirical Studies,Paper,Hairy Slices: Evaluating the Perceptual Effectiveness of Cutting Plane Glyphs for 3D Vector Fields,"Three-dimensional vector fields are common datasets throughout the sciences. Visualizing these fields is inherently difficult due to issues such as visual clutter and self-occlusion. Cutting planes are often used to overcome these issues by presenting more manageable slices of data. The existing literature provides many techniques for visualizing the flow through these cutting planes; however, there is a lack of empirical studies focused on the underlying perceptual cues that make popular techniques successful. This paper presents a quantitative human factors study that evaluates static monoscopic depth and orientation cues in the context of cutting plane glyph designs for exploring and analyzing 3D flow fields. The goal of the study was to ascertain the relative effectiveness of various techniques for portraying the direction of flow through a cutting plane at a given point, and to identify the visual cues and combinations of cues involved, and how they contribute to accurate performance. It was found that increasing the dimensionality of line-based glyphs into tubular structures enhances their ability to convey orientation through shading, and that increasing their diameter intensifies this effect. These tube-based glyphs were also less sensitive to visual clutter issues at higher densities. Adding shadows to lines was also found to increase perception of flow direction. Implications of the experimental results are discussed and extrapolated into a number of guidelines for designing more perceptually effective glyphs for 3D vector field visualizations.",https://europepmc.org/article/MED/278752121,Empirical Studies,Paper,Familiarity Vs Trust: A Comparative Study of Domain Scientists’ Trust in Visual Analytics and Conventional Analysis Methods,"Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.",https://ieeexplore.ieee.org/document/75361061,Empirical Studies,Paper,Modeling Color Difference for Visualization Design,"Color is frequently used to encode values in visualizations. For color encodings to be effective, the mapping between colors and values must preserve important differences in the data. However, most guidelines for effective color choice in visualization are based on either color perceptions measured using large, uniform fields in optimal viewing environments or on qualitative intuitions. These limitations may cause data misinterpretation in visualizations, which frequently use small, elongated marks. Our goal is to develop quantitative metrics to help people use color more effectively in visualizations. We present a series of crowdsourced studies measuring color difference perceptions for three common mark types: points, bars, and lines. Our results indicate that peoples' abilities to perceive color differences varies significantly across mark types. Probabilistic models constructed from the resulting data can provide objective guidance for designers, allowing them to anticipate viewer perceptions in order to inform effective encoding design.",https://pubmed.ncbi.nlm.nih.gov/28866544/2,Area 2,Description,,"This area encompasses all forms of application-focused research, which may aim to solve an application-motivated technical problem, to formulate the best practice in working with domain experts to transform general-purpose visualization technology to domain-specific solutions, to design and develop visualization systems and visual analytics workflows for supporting individual applications, or to gain insight into how to adapt and optimize visualization technology to support the users in a particular application domain. The technical solutions reported in this area are mostly application-specific and usually developed in collaboration with domain experts. These solutions can be in different forms, such as designs of visual representations and interaction techniques, descriptions of algorithms and techniques for data transformation, prototypes of visualization hardware and software, specifications of workflows and best practice, or design studies. Application papers underline the impact and importance of the field beyond the VIS research community itself.",2,Domains,Description,,"The use of visualization and visual analytics spreads across essentially all areas research and is relevant to commercial entities as well as non-profit and governmental agencies. In some areas the use has reached a high level of maturity whereas in other domains visualization is emerging as a new and essential component in the workflow. VIS welcomes submissions related to application domains spanning all existing, emerging and potential domains.",2,Technical Solution,Description,,"visual representations, interaction techniques, algorithms, techniques, hardware prototypes, software prototypes, integrated workflows, recommended working practice, etc.",2,Insight,Description,,"success stories and failures about applying visualization technology in practice, achievements of multidisciplinary research projects, benefits gained from collaboration with domain experts, and guidelines resulting from application-focused design studies.",2,Domains,Paper,Visual Analysis and Dissemination of Scientific Literature Collections with SurVis,"Bibliographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.",https://pubmed.ncbi.nlm.nih.gov/26529698/2,Domains,Paper,Lineage: Visualizing multivariate clinical data in genealogy graphs,"The majority of diseases that are a significant challenge for public and individual heath are caused by a combination of hereditary and environmental factors. In this paper we introduce Lineage, a novel visual analysis tool designed to support domain experts who study such multifactorial diseases in the context of genealogies. Incorporating familial relationships between cases with other data can provide insights into shared genomic variants and shared environmental exposures that may be implicated in such diseases. We introduce a data and task abstraction, and argue that the problem of analyzing such diseases based on genealogical, clinical, and genetic data can be mapped to a multivariate graph visualization problem. The main contribution of our design study is a novel visual representation for tree-like, multivariate graphs, which we apply to genealogies and clinical data about the individuals in these families. We introduce data-driven aggregation methods to scale to multiple families. By designing the genealogy graph layout to align with a tabular view, we are able to incorporate extensive, multivariate attributes in the analysis of the genealogy without cluttering the graph. We validate our designs by conducting case studies with our domain collaborators.",https://vdl.sci.utah.edu/publications/2018_tvcg_lineage/2,Domains,Paper,In Situ Distribution Guided Analysis and Visualization of Transonic Jet Engine Simulations,"Study of flow instability in turbine engine compressors is crucial to understand the inception and evolution of engine stall. Aerodynamics experts have been working on detecting the early signs of stall in order to devise novel stall suppression technologies. A state-of-the-art Navier-Stokes based, time-accurate computational fluid dynamics simulator, TURBO, has been developed in NASA to enhance the understanding of flow phenomena undergoing rotating stall. Despite the proven high modeling accuracy of TURBO, the excessive simulation data prohibits post-hoc analysis in both storage and I/O time. To address these issues and allow the expert to perform scalable stall analysis, we have designed an in situ distribution guided stall analysis technique. Our method summarizes statistics of important properties of the simulation data in situ using a probabilistic data modeling scheme. This data summarization enables statistical anomaly detection for flow instability in post analysis, which reveals the spatiotemporal trends of rotating stall for the expert to conceive new hypotheses. Furthermore, the verification of the hypotheses and exploratory visualization using the summarized data are realized using probabilistic visualization techniques such as uncertain isocontouring. Positive feedback from the domain scientist has indicated the efficacy of our system in exploratory stall analysis.",https://pubmed.ncbi.nlm.nih.gov/27875195/2,Technical Solution,Paper,HiPiler: Visual Exploration of Large Genome Interaction Matrices with Interactive Small Multiples,"This paper presents an interactive visualization interface-HiPiler-for the exploration and visualization of regions-of-interest in large genome interaction matrices. Genome interaction matrices approximate the physical distance of pairs of regions on the genome to each other and can contain up to 3 million rows and columns with many sparse regions. Regions of interest (ROIs) can be defined, e.g., by sets of adjacent rows and columns, or by specific visual patterns in the matrix. However, traditional matrix aggregation or pan-and-zoom interfaces fail in supporting search, inspection, and comparison of ROIs in such large matrices. In HiPiler, ROIs are first-class objects, represented as thumbnail-like ""snippets"". Snippets can be interactively explored and grouped or laid out automatically in scatterplots, or through dimension reduction methods. Snippets are linked to the entire navigable genome interaction matrix through brushing and linking. The design of HiPiler is based on a series of semi-structured interviews with 10 domain experts involved in the analysis and interpretation of genome interaction matrices. We describe six exploration tasks that are crucial for analysis of interaction matrices and demonstrate how HiPiler supports these tasks. We report on a user study with a series of data exploration sessions with domain experts to assess the usability of HiPiler as well as to demonstrate respective findings in the data.",https://pubmed.ncbi.nlm.nih.gov/28866592/2,Technical Solution,Paper,Globe Browsing: Contextualized Spatio-Temporal Planetary Surface Visualization,"Results of planetary mapping are often shared openly for use in scientific research and mission planning. In its raw format, however, the data is not accessible to non-experts due to the difficulty in grasping the context and the intricate acquisition process. We present work on tailoring and integration of multiple data processing and visualization methods to interactively contextualize geospatial surface data of celestial bodies for use in science communication. As our approach handles dynamic data sources, streamed from online repositories, we are significantly shortening the time between discovery and dissemination of data and results. We describe the image acquisition pipeline, the pre-processing steps to derive a 2.5D terrain, and a chunked level-of-detail, out-of-core rendering approach to enable interactive exploration of global maps and high-resolution digital terrain models. The results are demonstrated for three different celestial bodies. The first case addresses high-resolution map data on the surface of Mars. A second case is showing dynamic processes, such as concurrent weather conditions on Earth that require temporal datasets. As a final example we use data from the New Horizons spacecraft which acquired images during a single flyby of Pluto. We visualize the acquisition process as well as the resulting surface data. Our work has been implemented in the OpenSpace software [8], which enables interactive presentations in a range of environments such as immersive dome theaters, interactive touch tables, and virtual reality headsets.",https://pubmed.ncbi.nlm.nih.gov/28866505/2,Insight,Paper,Activity-Centered Domain Characterization for Problem-Driven Scientific Visualization,"Although visualization design models exist in the literature in the form of higher-level methodological frameworks, these models do not present a clear methodological prescription for the domain characterization step. This work presents a framework and end-to-end model for requirements engineering in problem-driven visualization application design. The framework and model are based on the activity-centered design paradigm, which is an enhancement of human-centered design. The proposed activity-centered approach focuses on user tasks and activities, and allows an explicit link between the requirements engineering process with the abstraction stage-and its evaluation-of existing, higher-level visualization design models. In a departure from existing visualization design models, the resulting model: assigns value to a visualization based on user activities; ranks user tasks before the user data; partitions requirements in activity-related capabilities and nonfunctional characteristics and constraints; and explicitly incorporates the user workflows into the requirements process. A further merit of this model is its explicit integration of functional specifications, a concept this work adapts from the software engineering literature, into the visualization design nested model. A quantitative evaluation using two sets of interdisciplinary projects supports the merits of the activity-centered model. The result is a practical roadmap to the domain characterization step of visualization design for problem-driven data visualization. Following this domain characterization model can help remove a number of pitfalls that have been identified multiple times in the visualization design literature.",https://pubmed.ncbi.nlm.nih.gov/28866550/2,Insight,Paper,Bridging From Goals to Tasks with Design Study Analysis Reports,"Visualization researchers and practitioners engaged in generating or evaluating designs are faced with the difficult problemof transforming the questions asked and actions taken by target users from domain-specific language and context into more abstractforms. Existing abstract task classifications aim to provide support for this endeavour by providing a carefully delineated suite of actions.Our experience is that this bottom-up approach is part of the challenge: low-level actions are difficult to interpret without a higher-levelcontext of analysis goals and the analysis process. To bridge this gap, we propose a framework based on analysis reports derived fromopen-coding 20 design study papers published at IEEE InfoVis 2009-2015, to build on the previous work of abstractions that collectivelyencompass a broad variety of domains. The framework is organized in two axes illustrated by nine analysis goals. It helps situate theanalysis goals by placing each goal under axes of specificity (Explore, Describe, Explain, Confirm) and number of data populations(Single, Multiple). The single-population types are Discover Observation, Describe Observation, Identify Main Cause, and CollectEvidence. The multiple-population types are Compare Entities, Explain Differences, and Evaluate Hypothesis. Each analysis goal isscoped by an input and an output and is characterized by analysis steps reported in the design study papers. We provide examplesof how we and others have used the framework in a top-down approach to abstracting domain problems: visualization designers orresearchers first identify the analysis goals of each unit of analysis in an analysis stream, and then encode the individual steps usingexisting task classifications with the context of the goal, the level of specificity, and the number of populations involved in the analysis.",https://research.tableau.com/sites/default/files/GoalsToTasks.pdf3,Area 3,Description,,"This area focuses on the themes of building systems, algorithms for rendering, and alternate input and output modalities. Papers submitted to this area may present new visualization system architectures, support different computing platforms and development environments, or exploit commodity and specialized hardware devices for either rendering or interaction modalities beyond the desktop. The rendering theme includes algorithms and techniques both in software and through hardware acceleration, and also algorithms for graph layout and label placement.",3,  Computing Platforms,Description,,"commodity hardware, GPU, HPC, energy efficient visualization algorithms and hardware, etc.",3,  Visualization Environments,Description,,"non-immersive and immersive environments, desktop, mobile, web-based, VR/MR/AR, dome theaters, CAVEs, physicalization, remote collaboration, etc.",3,  Display Hardware and Output Devices,Description,,"large and small displays, stereo displays, volumetric displays, 2D/3D printing, non-visual devices, etc.",3,  Interaction Modalities,Description,,"touch, pen, speech, gesture, haptics, etc.",3,  Development Environments,Description,,"programming languages, software libraries, authoring systems, visualization toolkits, software frameworks for integration and interoperability, etc.",3,  Processing Paradigms,Description,,"parallel, distributed, out-of-core, progressive, streaming, in situ, in transit, etc.",3,  Engineering Visualization Systems,Description,,"visualization system lifecycle, testing, performance analysis, verification, validation, etc.",3,  Visualization Systems,Description,,"general-purpose and application-specific plug-ins, apps, tools, systems, multi-system workflows, etc.",3,  Data and Software Resources,Description,,"open data, open source software, benchmark data, reproducibility, authentication, etc.",3,  Rendering Techniques,Description,,"surface rendering, volume rendering, point-cloud rendering, line-cloud rendering, global illumination, stylized rendering, transfer functions, etc.",3,  Lighting and Shading Models,Description,,"volume rendering integrals, spectral rendering, learning lighting and shading models from real-world data.",3,  Placement Techniques,Description,,"object placement, graph layout, word/tag cloud, etc.",3,  Other Synthesis Techniques,Description,,"fabrication, sonification, haptic feedback, etc.",3,  Computing Platforms,Paper,Cross-Platform Ubiquitous Volume Rendering Using Programmable Shaders in VTK for Scientific and Medical Visualization,"The visualization toolkit (VTK) is a popular cross-platform, open source toolkit for scientific and medical data visualization, processing, and analysis. It supports a wide variety of data formats, algorithms, and rendering techniques for both polygonal and volumetric data. In particular, VTK's volume rendering module has long provided a comprehensive set of features such as plane clipping, color and opacity transfer functions, lighting, and other controls needed for visualization. However, due to VTK's legacy OpenGL backend and its reliance on a deprecated API, the system did not take advantage of the latest improvements in graphics hardware or the flexibility of a programmable pipeline. Additionally, this dependence on an antiquated pipeline posed restrictions when running on emerging computing platforms, thereby limiting its overall applicability. In response to these shortcomings, the VTK community developed a new and improved volume rendering module, which not only provides a modern graphics processing unit-based implementation, but also augments its capabilities with new features such as fast volume clipping, gradient-magnitude-based opacity modulation, render to texture, and hardware-based volume picking.",https://ieeexplore.ieee.org/document/86636503,  Visualization Environments,Paper,Designing for Mobile and Immersive Visual Analytics in the Field,"Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face data- and platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data's utility for various field operations and new directions for visual analytics tools to transform fieldwork.",https://arxiv.org/abs/1908.006803,"  M. Kraus, N. K. Weiler, D. Oelke, J. Kehrer, D. Keim, J. Fuchs. ""The Impact of Immersion on Cluster Identification Tasks"",  IEEE Transactions on Visualization and Computer Graphics, 26(1)",Paper,The Impact of Immersion on Cluster Identification Tasks,"Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.",https://pubmed.ncbi.nlm.nih.gov/31536002/3,  Display Hardware and Output Devices,Paper,VisTiles: Coordinating and Combining Co-located Mobile Devices for Visual Data Exploration,"We presentVISTILES, a conceptual framework that uses a set of mobile devices to distribute and coordinate visualizationviews for the exploration of multivariate data. In contrast to desktop-based interfaces for information visualization, mobile devicesoffer the potential to provide a dynamic and user-defined interface supporting co-located collaborative data exploration with differentindividual workflows. As part of our framework, we contribute concepts that enable users to interact with coordinated & multiple views(CMV) that are distributed across several mobile devices. The major components of the framework are: (i)dynamic and flexible layoutsfor CMVfocusing on the distribution of views and (ii) an interaction concept forsmart adaptations and combinations of visualizationsutilizing explicit side-by-side arrangements of devices. As a result, users can benefit from the possibility to combine devices andorganize them in meaningful spatial layouts. Furthermore, we present a web-based prototype implementation as a specific instance ofour concepts. This implementation provides a practical application case enabling users to explore a multivariate data collection. Wealso illustrate the design process including feedback from a preliminary user study, which informed the design of both the concepts andthe final prototype.",https://imld.de/cnt/uploads/Langner_VisTiles_InfoVis17.pdf3,  Interaction Modalities,Paper,A Lightweight Tangible 3D Interface for Interactive Visualization of Thin Fiber Structures,"We present a prop-based, tangible interface for 3D interactive visualization of thin fiber structures. These data are com-monly found in current bioimaging datasets, for example second-harmonic generation microscopy of collagen fibers in tissue. Ourapproach uses commodity visualization technologies such as a depth sensing camera and low-cost 3D display. Unlike most currentuses of these emerging technologies in the games and graphics communities, we employ the depth sensing camera to create afish-tank stereoscopic virtual reality system at the scientist’s desk that supports tracking of small-scale gestures with objects alreadyfound in the work space. We apply the new interface to the problem of interactive exploratory visualization of three-dimensional thinfiber data. A critical task for the visual analysis of these data is understanding patterns in fiber orientation throughout a volume.Theinterface enables a new, fluid style of data exploration and fiber orientation analysis by using props to provide needed passive-hapticfeedback, making 3D interactions with these fiber structures more controlled. We also contribute a low-level algorithm for extractingfiber centerlines from volumetric imaging. The system was designed and evaluated with two biophotonic experts who currently use itin their lab. As compared to typical practice within their field, the new visualization system provides a more effective way to examineand understand the 3D bioimaging datasets they collect.",http://bret-jackson.com/papers/vis13-propbased-fiber-vis.pdf3,  Development Environments,Paper,Vega-lite: A grammar of interactive graphics,"We present Vega-Lite, a high-level grammar that enables rapid specification of interactive data visualizations. Vega-Lite combines a traditional grammar of graphics, providing visual encoding rules and a composition algebra for layered and multi-view displays, with a novel grammar of interaction. Users specify interactive semantics by composing selections. In Vega-Lite, a selection is an abstraction that defines input event processing, points of interest, and a predicate function for inclusion testing. Selections parameterize visual encodings by serving as input data, defining scale extents, or by driving conditional logic. The Vega-Lite compiler automatically synthesizes requisite data flow and event handling logic, which users can override for further customization. In contrast to existing reactive specifications, Vega-Lite selections decompose an interaction design into concise, enumerable semantic units. We evaluate Vega-Lite through a range of examples, demonstrating succinct specification of both customized interaction methods and common techniques such as panning, zooming, and linked selection.",https://pubmed.ncbi.nlm.nih.gov/27875150/3,  Processing Paradigms,Paper,VTK-m: Accelerating the Visualization Toolkit for Massively Threaded Architectures,"One of the most critical challenges for high-performance computing (HPC) scientific visualization is execution on massively threaded processors. Of the many fundamental changes we are seeing in HPC systems, one of the most profound is a reliance on new processor types optimized for execution bandwidth over latency hiding. Our current production scientific visualization software is not designed for these new types of architectures. To address this issue, the VTK-m framework serves as a container for algorithms, provides flexible data representation, and simplifies the design of visualization algorithms on new and future computer architecture.",http://www.sci.utah.edu/publications/Mor2016b/07466740.pdf3,  Engineering Visualization Systems,Paper,"Keeping Multiple Views Consistent: Constraints, Validations and Exceptions in Visualization Authoring","Visualizations often appear in multiples, either in a single display (e.g., small multiples, dashboard) or across time or space (e.g., slideshow, set of dashboards). However, existing visualization design guidelines typically focus on single rather than multiple views. Solely following these guidelines can lead to effective yet inconsistent views (e.g., the same field has different axes domains across charts), making interpretation slow and error-prone. Moreover, little is known how consistency balances with other design considerations, making it difficult to incorporate consistency mechanisms in visualization authoring software. We present a wizard-of-oz study in which we observed how Tableau users achieve and sacrifice consistency in an exploration-to-presentation visualization design scenario. We extend (from our prior work) a set of encoding-specific constraints defining consistency across multiple views. Using the constraints as a checklist in our study, we observed cases where participants spontaneously maintained consistent encodings and warned cases where consistency was overlooked. In response to the warnings, participants either revised views for consistency or stated why they thought consistency should be overwritten. We categorize participants' actions and responses as constraint validations and exceptions, depicting the relative importance of consistency and other design considerations under various circumstances (e.g., data cardinality, available encoding resources, chart layout). We discuss automatic consistency checking as a constraint-satisfaction problem and provide design implications for communicating inconsistencies to users.",https://ieeexplore.ieee.org/abstract/document/80176513,  Visualization Systems,Paper,OSPRay – A CPU Ray Tracing Framework for Scientific Visualization,"Scientific data is continually increasing in complexity, variety and size, making efficient visualization and specifically rendering an ongoing challenge. Traditional rasterization-based visualization approaches encounter performance and quality limitations, particularly in HPC environments without dedicated rendering hardware. In this paper, we present OSPRay, a turn-key CPU ray tracing framework oriented towards production-use scientific visualization which can utilize varying SIMD widths and multiple device backends found across diverse HPC resources. This framework provides a high-quality, efficient CPU-based solution for typical visualization workloads, which has already been integrated into several prevalent visualization packages. We show that this system delivers the performance, high-level API simplicity, and modular device support needed to provide a compelling new rendering framework for implementing efficient scientific visualization workflows.",https://ieeexplore.ieee.org/document/75395993,  Visualization Systems,Paper,Inviwo - A Visualization System with Usage Abstraction Levels,"The complexity of today's visualization applications demands specific visualization systems tailored for the development of these applications. Frequently, such systems utilize levels of abstraction to improve the application development process, for instance by providing a data flow network editor. Unfortunately, these abstractions result in several issues, which need to be circumvented through an abstraction-centered system design. Often, a high level of abstraction hides low level details, which makes it difficult to directly access the underlying computing platform, which would be important to achieve an optimal performance. Therefore, we propose a layer structure developed for modern and sustainable visualization systems allowing developers to interact with all contained abstraction levels. We refer to this interaction capabilities as usage abstraction levels, since we target application developers with various levels of experience. We formulate the requirements for such a system, derive the desired architecture, and present how the concepts have been exemplary realized within the Inviwo visualization system. Furthermore, we address several specific challenges that arise during the realization of such a layered architecture, such as communication between different computing platforms, performance centered encapsulation, as well as layer-independent development by supporting cross layer documentation and debugging capabilities.",https://ieeexplore.ieee.org/document/87305133,  Data and Software Resources,Paper,vispubdata.org: A Metadata Collection about IEEE Visualization (VIS) Publications,"We have created and made available to all a dataset with information about every paper that has appeared at the IEEE Visualization (VIS) set of conferences: InfoVis, SciVis, VAST, and Vis. The information about each paper includes its title, abstract, authors, and citations to other papers in the conference series, among many other attributes. This article describes the motivation for creating the dataset, as well as our process of coalescing and cleaning the data, and a set of three visualizations we created to facilitate exploration of the data. This data is meant to be useful to the broad data visualization community to help understand the evolution of the field and as an example document collection for text data visualization research.",https://ieeexplore.ieee.org/document/75837083,  Rendering Techniques,Paper,SparseLeap: Efficient Empty Space Skipping for Large-Scale Volume Rendering,"Recent advances in data acquisition produce volume data of very high resolution and large size, such as terabyte-sized microscopy volumes. These data often contain many fine and intricate structures, which pose huge challenges for volume rendering, and make it particularly important to efficiently skip empty space. This paper addresses two major challenges: (1) The complexity of large volumes containing fine structures often leads to highly fragmented space subdivisions that make empty regions hard to skip efficiently. (2) The classification of space into empty and non-empty regions changes frequently, because the user or the evaluation of an interactive query activate a different set of objects, which makes it unfeasible to pre-compute a well-adapted space subdivision. We describe the novel SparseLeap method for efficient empty space skipping in very large volumes, even around fine structures. The main performance characteristic of SparseLeap is that it moves the major cost of empty space skipping out of the ray-casting stage. We achieve this via a hybrid strategy that balances the computational load between determining empty ray segments in a rasterization (object-order) stage, and sampling non-empty volume data in the ray-casting (image-order) stage. Before ray-casting, we exploit the fast hardware rasterization of GPUs to create a ray segment list for each pixel, which identifies non-empty regions along the ray. The ray-casting stage then leaps over empty space without hierarchy traversal. Ray segment lists are created by rasterizing a set of fine-grained, view-independent bounding boxes. Frame coherence is exploited by re-using the same bounding boxes unless the set of active objects changes. We show that SparseLeap scales better to large, sparse data than standard octree empty space skipping.",https://pubmed.ncbi.nlm.nih.gov/28866532/3,  Lighting and Shading Models,Paper,Correlated Photon Mapping for Interactive Global Illumination of Time-Varying Volumetric Data,"We present a method for interactive global illumination of both static and time-varying volumetric data based on reduction of the overhead associated with re-computation of photon maps. Our method uses the identification of photon traces invariant to changes of visual parameters such as the transfer function (TF), or data changes between time-steps in a 4D volume. This lets us operate on a variant subset of the entire photon distribution. The amount of computation required in the two stages of the photon mapping process, namely tracing and gathering, can thus be reduced to the subset that are affected by a data or visual parameter change. We rely on two different types of information from the original data to identify the regions that have changed. A low resolution uniform grid containing the minimum and maximum data values of the original data is derived for each time step. Similarly, for two consecutive time-steps, a low resolution grid containing the difference between the overlapping data is used. We show that this compact metadata can be combined with the transfer function to identify the regions that have changed. Each photon traverses the low-resolution grid to identify if it can be directly transferred to the next photon distribution state or if it needs to be recomputed. An efficient representation of the photon distribution is presented leading to an order of magnitude improved performance of the raycasting step. The utility of the method is demonstrated in several examples that show visual fidelity, as well as performance. The examples show that visual quality can be retained when the fraction of retraced photons is as low as 40%-50%.",https://pubmed.ncbi.nlm.nih.gov/27514045/3,  Placement Techniques,Paper,Hola: Human-like orthogonal network layout,"Over the last 50 years a wide variety of automatic network layout algorithms have been developed. Some are fast heuristictechniques suitable for networks with hundreds of thousands of nodes while others are multi-stage frameworks for higher-quality layoutof smaller networks. However, despite decades of research currently no algorithm produces layout of comparable quality to that ofa human. We give a new “human-centred” methodology for automatic network layout algorithm design that is intended to overcomethis deficiency. User studies are first used to identify the aesthetic criteria algorithms should encode, then an algorithm is developedthat is informed by these criteria and finally, a follow-up study evaluates the algorithm output. We have used this new methodology todevelop an automatic orthogonal network layout method, HOLA, that achieves measurably better (by user study) layout than the bestavailable orthogonal layout algorithm and which produces layouts of comparable quality to those produced by hand.",https://ialab.it.monash.edu/~dwyer/papers/hola2015.pdf3,  Other Synthesis Techniques,Paper,Activity Sculptures: Exploring the Impact of Physical Visualizations on Running Activity,"Data sculptures are a promising type of visualizations in which data is given a physical form. In the past, they have mostly been used for artistic, communicative or educational purposes, and designers of data sculptures argue that in such situations, physical visualizations can be more enriching than pixel-based visualizations. We present the design of Activity Sculptures: data sculptures of running activity. In a three-week field study we investigated the impact of the sculptures on 14 participants' running activity, the personal and social behaviors generated by the sculptures, as well as participants' experiences when receiving these individual physical tokens generated from the specific data of their runs. The physical rewards generated curiosity and personal experimentation but also social dynamics such as discussion on runs or envy/competition. We argue that such passive (or calm) visualizations can complement nudging and other mechanisms of persuasion with a more playful and reflective look at ones' activity.",https://pubmed.ncbi.nlm.nih.gov/26356934/3,  Other Synthesis Techniques,Paper,SonifEye: Sonification of Visual Information Using Physical Modeling Sound Synthesis,"Sonic interaction as a technique for conveying information has advantages over conventional visual augmented reality methods specially when augmenting the visual field with extra information brings distraction. Sonification of knowledge extracted by applying computational methods to sensory data is a well-established concept. However, some aspects of sonic interaction design such as aesthetics, the cognitive effort required for perceiving information, and avoiding alarm fatigue are not well studied in literature. In this work, we present a sonification scheme based on employment of physical modeling sound synthesis which targets focus demanding tasks requiring extreme precision. Proposed mapping techniques are designed to require minimum training for users to adapt to and minimum mental effort to interpret the conveyed information. Two experiments are conducted to assess the feasibility of the proposed method and compare it against visual augmented reality in high precision tasks. The observed quantitative results suggest that utilizing sound patches generated by physical modeling achieve the desired goal of improving the user experience and general task performance with minimal training.",https://ieeexplore.ieee.org/document/80073274,Area 4,Description,,"This area focuses on the design of visual representations and interaction techniques for different types of data, users, and visualization tasks. In principle, the data concerned can be of any data types, such as spatial or non-spatial; continuous or discrete; statistic, temporal or streaming; numerical, textual or imagery, etc. The user concerned can be from any user groups (e.g., scientists, scholars, students, analysts, administrators, or the general public) and of any level of visualization literacy and skills. The tasks concerned can be of any operational needs, such as effective information dissemination, rapid data observation, and explorative information seeking. The visual representations concerned can be of elementary encoding (e.g., visual channels, statistical graphics) as well as complex visual mapping (e.g., spatiotemporal data visualization and coordinated multiple views), and can be in visual as well as non-visual forms for enabling visualization via different human sensory devices. The interaction techniques can be based on traditional WIMP (windows, icons, menus, and pointers) and direct manipulation. Papers submitted to this area are normally expected to emphasize their novel contributions in terms of the design of visual representations and interaction techniques, while the work may also discuss the related hardware and software components for data transformation, image synthesis and displays, interaction, and immersion (see also Areas 3 and 5).",4,  Visual Channels,Description,,"geometric channels (e.g., location, size, orientation, shape, etc.), optical channels (e.g., color, opacity, shading, motion, etc.), topological and relational channels (e.g., connection, overlapping, etc.), and semantic channels (e.g., number, text, glyph, etc.).",4,  Visual Representations,Description,,"for textual data, tabular data, relational data (e.g., hierarchy, tree, set, graph/network), geospatial data, temporal data, imagery data, geometric data (mesh-, point-, line-, curve-based data), field-based data (e.g., volumetric, vector, and tensor field), corpus data, multi-type data, uncertain and missing data, models, functions, and procedures (e.g., algorithms and software), etc. in raw, filtered, or transformed (e.g., aggregated) form.",4,  Interaction Techniques,Description,,"UI design for visualization, zoom and navigation, magic lens, query-based exploration, direct manipulation, interactive deformation,natural interaction, user-adaptive interaction,  interoperation between interaction and visualization tasks, editing tools, collaborative visualization, etc.",4,  Visual Communication Techniques,Description,,"focus+context design, illustrative and explanatory visualization, stylized visual representations, storytelling and narrative visualization, textual annotation for visualization, etc.",4,  Intelligent Visualization and Interaction,Description,,"Automated visualization generation, mixed-initiative visual interaction, learning UI models for automated capabilities in visualization systems.",4,  Technical Discourses on Visual Representations and Interaction Techniques,Description,,"visual and interactional metaphors, scalability of visual mapping, and interaction costs, 2D vs. 3D representations, static vs. animated representations, visualization literacy, etc.",4,  Visual Channels,Paper,"The Good, the Bad, and the Ugly: A Theoretical Framework for the Assessment of Continuous Colormaps","A myriad of design rules for what constitutes a “good” colormap can be found in the literature. Some common rules include order, uniformity, and high discriminative power. However, the meaning of many of these terms is often ambiguous or open to interpretation. At times, different authors may use the same term to describe different concepts or the same rule is described by varying nomenclature. These ambiguities stand in the way of collaborative work, the design of experiments to assess the characteristics of colormaps, and automated colormap generation. In this paper, we review current and historical guidelines for colormap design. We propose a specified taxonomy and provide unambiguous mathematical definitions for the most common design rules.",https://ieeexplore.ieee.org/document/80176534,  Visual Representations,Paper,Time curves: Folding time to visualize patterns of temporal evolution in data,"We introduce time curves as a general approach for visualizing patterns of evolution in temporal data. Examples of such patterns include slow and regular progressions, large sudden changes, and reversals to previous states. These patterns can be of interest in a range of domains, such as collaborative document editing, dynamic network analysis, and video analysis. Time curves employ the metaphor of folding a timeline visualization into itself so as to bring similar time points close to each other. This metaphor can be applied to any dataset where a similarity metric between temporal snapshots can be defined, thus it is largely datatype-agnostic. We illustrate how time curves can visually reveal informative patterns in a range of different datasets.",https://pubmed.ncbi.nlm.nih.gov/26529718/4,  Interaction Techniques,Paper,Applying Pragmatics Principles for Interaction with Visual Analytics.,"Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.",https://pubmed.ncbi.nlm.nih.gov/28866554/4,  Visual Communication Techniques,Paper,Timelines Revisited: A Design Space and Considerations for Expressive Storytelling,"There are many ways to visualize event sequences as timelines. In a storytelling context where the intent is to convey multiple narrative points, a richer set of timeline designs may be more appropriate than the narrow range that has been used for exploratory data analysis by the research community. Informed by a survey of 263 timelines, we present a design space for storytelling with timelines that balances expressiveness and effectiveness, identifying 14 design choices characterized by three dimensions: representation, scale, and layout. Twenty combinations of these choices are viable timeline designs that can be matched to different narrative points, while smooth animated transitions between narrative points allow for the presentation of a cohesive story, an important aspect of both interactive storytelling and data videos. We further validate this design space by realizing the full set of viable timeline designs and transitions in a proof-of-concept sandbox implementation that we used to produce seven example timeline stories. Ultimately, this work is intended to inform and inspire the design of future tools for storytelling with timelines.",https://pubmed.ncbi.nlm.nih.gov/28113509/4,  Intelligent Visualization and Interaction,Paper,A Grammar-based Approach for Modeling User Interactions and Generating Suggestions During the Data Exploration Process.,"Despite the recent popularity of visual analytics focusing on big data, little is known about how to support users that use visualization techniques to explore multi-dimensional datasets and accomplish specific tasks. Our lack of models that can assist end-users during the data exploration process has made it challenging to learn from the user's interactive and analytical process. The ability to model how a user interacts with a specific visualization technique and what difficulties they face are paramount in supporting individuals with discovering new patterns within their complex datasets. This paper introduces the notion of visualization systems understanding and modeling user interactions with the intent of guiding a user through a task thereby enhancing visual data exploration. The challenges faced and the necessary future steps to take are discussed; and to provide a working example, a grammar-based model is presented that can learn from user interactions, determine the common patterns among a number of subjects using a K-Reversible algorithm, build a set of rules, and apply those rules in the form of suggestions to new users with the goal of guiding them along their visual analytic process. A formal evaluation study with 300 subjects was performed showing that our grammar-based model is effective at capturing the interactive process followed by users and that further research in this area has the potential to positively impact how users interact with a visualization system.",https://pubmed.ncbi.nlm.nih.gov/27514057/4,  Intelligent Visualization and Interaction,Paper,Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration,"Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.",https://pubmed.ncbi.nlm.nih.gov/27875149/4,  Technical Discourses,Paper,Active Reading of Visualizations.,"We investigate whether the notion of active reading for text might be usefully applied to visualizations. Through a qualitative study we explored whether people apply observable active reading techniques when reading paper-based node-link visualizations. Participants used a range of physical actions while reading, and from these we synthesized an initial set of active reading techniques for visualizations. To learn more about the potential impact such techniques may have on visualization reading, we implemented support for one type of physical action from our observations (making freeform marks) in an interactive node-link visualization. Results from our quantitative study of this implementation show that interactive support for active reading techniques can improve the accuracy of performing low-level visualization tasks. Together, our studies suggest that the active reading space is ripe for research exploration within visualization and can lead to new interactions that make for a more flexible and effective visualization reading experience.",https://ieeexplore.ieee.org/document/80176065,Area 5,Description,,"This area focuses on the algorithms and techniques that transform data from one form to another to enable effective and efficient visual mapping as required by the intended visual representations. In principle, the source and destination data can be of any data types, such as spatial or non-spatial; continue or discrete; statistical, temporal or streaming; numerical, textual or imagery, etc. Such data transformation, which may sometimes be referred to as wrangling or munging in some other fields, may include extracting information from the source data (e.g., surface extraction from volume data, and network construction from textual data), integrating data from different sources (e.g., multi-modality registration), reorganizing data for efficient processing (e.g., hierarchical data representations), enriching data with additional information and functions (e.g., uncertainty analysis and label generation), and improving data quality and usability (e.g., data cleansing). Papers submitted to this area are normally expected to emphasize their novel contributions in terms of the algorithms and techniques for data transformation, while the work may also discuss the intended visual representations and their generation (see also Areas 3 and 4).",5,  Information Extraction and Data Abstraction,Description,,"keyword extraction, metadata extraction, surface extraction, feature extraction, pattern recognition, structural and semantic analysis, skeletonization, spatial abstraction, topological abstraction, temporal feature tracking, multi-material interfaces, etc.",5,  Data Integration,Description,,"multi-modality, multi-stage, and multi-level data registration, spatial and non-spatial data integration, multi-field representations, etc.",5,  Data Reorganization,Description,,"voxelization, triangularization, multi-resolution sampling and representations (e.g., discrete sampling, volumetric lattices, wavelet representations), spatial partitioning (e.g., octree, k-d tree, bounding volume), data segmentation, compressed data representations, frequency-domain representations, databases for query-based visualization, etc.",5,  Data Enrichment,Description,,"uncertainty analysis, deformable models, label generation, spatialization, etc.",5,  Data Wrangling and Improvement,Description,,"data wrangling, data re-shaping, data cleaning, data editing, data smoothing, and data modelling.",5,  Mathematical Frameworks for Data Transformation,Description,,"numerical analysis, computational geometry, topological analysis, graph theory, statistical analysis, probability theory, information theory, dimensionality reduction, etc.",5,  Machine Learning for Data Transformation,Description,,"automated discovery of data models and data transformation algorithms for visualization, learning-based parameter optimization of data models and data transformation algorithms for visualization, etc.",5,  Technical Discourses on Data Processing and Management in Visualization,Description,,"feature specification, data provenance, processing provenance, interactive processing, data synthesis, quality assurance, etc.",5,  Information Extraction and Data Abstraction,Paper,Jacobi Fiber Surfaces for Bivariate Reeb Space Computation,"This paper presents an efficient algorithm for the computation of the Reeb space of an input bivariate piecewise linear scalar function f defined on a tetrahedral mesh. By extending and generalizing algorithmic concepts from the univariate case to the bivariate one, we report the first practical, output-sensitive algorithm for the exact computation of such a Reeb space. The algorithm starts by identifying the Jacobi set of f, the bivariate analogs of critical points in the univariate case. Next, the Reeb space is computed by segmenting the input mesh along the new notion of Jacobi Fiber Surfaces, the bivariate analog of critical contours in the univariate case. We additionally present a simplification heuristic that enables the progressive coarsening of the Reeb space. Our algorithm is simple to implement and most of its computations can be trivially parallelized. We report performance numbers demonstrating orders of magnitude speedups over previous approaches, enabling for the first time the tractable computation of bivariate Reeb spaces in practice. Moreover, unlike range-based quantization approaches (such as the Joint Contour Net), our algorithm is parameter-free. We demonstrate the utility of our approach by using the Reeb space as a semi-automatic segmentation tool for bivariate data. In particular, we introduce continuous scatterplot peeling, a technique which enables the reduction of the cluttering in the continuous scatterplot, by interactively selecting the features of the Reeb space to project. We provide a VTK-based C++ implementation of our algorithm that can be used for reproduction purposes or for the development of new Reeb space based visualization techniques.",https://pubmed.ncbi.nlm.nih.gov/27875209/5,  Data Integration,Paper,Orko: Facilitating Multimodal Interaction for Visual Network Exploration and Analysis,"Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces.  Only re-cently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operatingonly independently.  Prior evaluations of natural language interfaces for visualization have indicated potential value in combining di-rect manipulation and natural language as complementary interaction techniques.  We hypothesize that truly multimodal interfacesfor visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulationinput, may provide an effective and engaging user experience.  Unfortunately, however, little work has been done in exploring suchmultimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system calledOrko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visual-ization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior researchexploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discussopportunities and challenges for future work in multimodal network visualization interfaces.",https://www.cc.gatech.edu/~stasko/papers/infovis17-orko.pdf5,  Data Wrangling and Improvement,Paper,Origraph: Interactive Network Wrangling.,"Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.",https://vdl.sci.utah.edu/publications/2019_vast_origraph/5,  Data Reorganization,Paper,Multiscale Visualization and Scale-Adaptive Modification of DNA Nanostructures.,"We present an approach to represent DNA nanostructures in varying forms of semantic abstraction, describe ways to smoothly transition between them, and thus create a continuous multiscale visualization and interaction space for applications in DNA nanotechnology. This new way of observing, interacting with, and creating DNA nanostructures enables domain experts to approach their work in any of the semantic abstraction levels, supporting both low-level manipulations and high-level visualization and modifications. Our approach allows them to deal with the increasingly complex DNA objects that they are designing, to improve their features, and to add novel functions in a way that no existing single-scale approach offers today. For this purpose we collaborated with DNA nanotechnology experts to design a set of ten semantic scales. These scales take the DNA's chemical and structural behavior into account and depict it from atoms to the targeted architecture with increasing levels of abstraction. To create coherence between the discrete scales, we seamlessly transition between them in a well-defined manner. We use special encodings to allow experts to estimate the nanoscale object's stability. We also add scale-adaptive interactions that facilitate the intuitive modification of complex structures at multiple scales. We demonstrate the applicability of our approach on an experimental use case. Moreover, feedback from our collaborating domain experts confirmed an increased time efficiency and certainty for analysis and modification tasks on complex DNA structures. Our method thus offers exciting new opportunities with promising applications in medicine and biotechnology.",https://pubmed.ncbi.nlm.nih.gov/28866510/5,  Data Enrichment,Paper,Uncertainty Visualization Using Copula-Based Analysis in Mixed Distribution Models,"Distributions are often used to model uncertainty in many scientific datasets. To preserve the correlation among thespatially sampled grid locations in the dataset, various standard multivariate distribution models have been proposed in visualizationliterature. These models treat each grid location as a univariate random variable which models the uncertainty at that location. Standardmultivariate distributions (both parametric and nonparametric) assume that all the univariate marginals are of the same type/family ofdistribution. But in reality, different grid locations show different statistical behavior which may not be modeled best by the same type ofdistribution. In this paper, we propose a new multivariate uncertainty modeling strategy to address the needs of uncertainty modelingin scientific datasets. Our proposed method is based on a statistically sound multivariate technique calledCopula, which makes itpossible to separate the process of estimating the univariate marginals and the process of modeling dependency, unlike the standardmultivariate distributions. The modeling flexibility offered by our proposed method makes it possible to design distribution fields whichcan have different types of distribution (Gaussian, Histogram, KDE etc.) at the grid locations, while maintaining the correlation structureat the same time. Depending on the results of various standard statistical tests, we can choose an optimal distribution representation ateach location, resulting in a more cost efficient modeling without significantly sacrificing on the analysis quality. To demonstrate theefficacy of our proposed modeling strategy, we extract and visualize uncertain features like isocontours and vortices in various realworld datasets. We also study various modeling criterion to help users in the task of univariate model selection.",https://subhashis.github.io/papers/copula-2018.pdf5,  Mathematical Frameworks for Data Transformation,Paper,On the Treatment of Field Quantities and Elemental Continuity in FEM Solutions.,"As the finite element method (FEM) and the finite volume method (FVM), both traditional and high-order variants, continue their proliferation into various applied engineering disciplines, it is important that the visualization techniques and corresponding data analysis tools that act on the results produced by these methods faithfully represent the underlying data. To state this in another way: the interpretation of data generated by simulation needs to be consistent with the numerical schemes that underpin the specific solver technology. As the verifiable visualization literature has demonstrated: visual artifacts produced by the introduction of either explicit or implicit data transformations, such as data resampling, can sometimes distort or even obfuscate key scientific features in the data. In this paper, we focus on the handling of elemental continuity, which is often only continuous or piecewise discontinuous, when visualizing primary or derived fields from FEM or FVM simulations. We demonstrate that traditional data handling and visualization of these fields introduce visual errors. In addition, we show how the use of the recently proposed line-SIAC filter provides a way of handling elemental continuity issues in an accuracy-conserving manner with the added benefit of casting the data in a smooth context even if the representation is element discontinuous.",https://pubmed.ncbi.nlm.nih.gov/28866517/5,  Technical Discourses on Data Processing and Management in Visualization,Paper,Rotation Invariant Vortices for Flow Visualization,"We propose a new class of vortex definitions for flows that are induced by rotating mechanical parts, such as stirring devices, helicopters, hydrocyclones, centrifugal pumps, or ventilators. Instead of a Galilean invariance, we enforce a rotation invariance, i.e., the invariance of a vortex under a uniform-speed rotation of the underlying coordinate system around a fixed axis. We provide a general approach to transform a Galilean invariant vortex concept to a rotation invariant one by simply adding a closed form matrix to the Jacobian. In particular, we present rotation invariant versions of the well-known Sujudi-Haimes, Lambda-2, and Q vortex criteria. We apply them to a number of artificial and real rotating flows, showing that for these cases rotation invariant vortices give better results than their Galilean invariant counterparts.",https://pubmed.ncbi.nlm.nih.gov/26390472/6,Area 6,Description,,"This area focuses on the design and optimization of integrated workflows for visual data analysis, knowledge discovery, decision support, machine learning, and other data intelligence tasks. It typically addresses technical problems that cannot be solved using solely machine-centric processes (e.g., statistics and algorithms) or solely human-centric processes (e.g., visualization and interaction). It may also address the need for using interactive visualization to improve the trust, interpretability, understanding of machine-centric processes and their underlying models and need for data intelligence workflows to benefit from theoretical models and empirical findings in cognition, e.g., in areas such as distributed, embodied, and enactive cognition. Hence papers submitted to this area are normally expected to feature an integrated approach. This area includes visualizations and visual analytic tools that show these models, or leverage them heavily for producing the visualization.",6,"  Integrated Workflows for Information Seeking, Knowledge Discovery, and Decision Making",Description,,"Typical technical problems may include information retrieval, multivariate and semantic search; classification, pattern recognition and clustering; similarity, correlation and causality analysis; spatiotemporal tracking and movement analysis; event and sequence analysis; multimedia data analysis; anomaly and change detection; relationship, association, hierarchy, network and structure analysis; intention and behavior analysis; factor analysis and dimensionality reduction; uncertainty and risk analysis; and so on.",6,  Integrated Workflows for Machine Learning,Description,,"Typical technical problems may include cleaning and labelling training data; assisting active learning or other semi-automated learning methods; facilitating model testing, evaluation and model comparison; supporting the analysis of learned models and learning processes; enabling model understanding, explanation, refinement, and steering; and monitoring the deployment of machine-learned models as well as other machine-centric processes.",6,  Workflow Optimization,Description,,"techniques, design patterns, and best practices for designing, developing, evaluating, and improving integrated data intelligence workflows. Methods for analysing and alleviating data biases, machine biases, and human biases.",6,  Knowledge-assisted Workflows,Description,,"knowledge acquisition, mixed-initiative workflows, real-time guidance and recommendation, provenance management and utilization, post-action review, knowledge sharing, and analyst training in visual data analysis.",6,"  Integrated Workflows for Information Seeking, Knowledge Discovery, and Decision Making",Paper,TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis.,"Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.",https://pubmed.ncbi.nlm.nih.gov/30136965/6,"  Integrated Workflows for Information Seeking, Knowledge Discovery, and Decision Making",Paper,A Visual Analytics Approach for Categorical Joint Distribution Reconstruction from Marginal Projections,"Oftentimes multivariate data are not available as sets of equally multivariate tuples, but only as sets of projections into subspaces spanned by subsets of these attributes. For example, one may find data with five attributes stored in six tables of two attributes each, instead of a single table of five attributes. This prohibits the visualization of these data with standard high-dimensional methods, such as parallel coordinates or MDS, and there is hence the need to reconstruct the full multivariate (joint) distribution from these marginal ones. Most of the existing methods designed for this purpose use an iterative procedure to estimate the joint distribution. With insufficient marginal distributions and domain knowledge, they lead to results whose joint errors can be large. Moreover, enforcing smoothness for regularizations in the joint space is not applicable if the attributes are not numerical but categorical. We propose a visual analytics approach that integrates both anecdotal data and human experts to iteratively narrow down a large set of plausible solutions. The solution space is populated using a Monte Carlo procedure which uniformly samples the solution space. A level-of-detail high dimensional visualization system helps the user understand the patterns and the uncertainties. Constraints that narrow the solution space can then be added by the user interactively during the iterative exploration, and eventually a subset of solutions with narrow uncertainty intervals emerges.",https://pubmed.ncbi.nlm.nih.gov/27514059/6,  Integrated Workflows for Machine Learning,Paper,Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow,"We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.",https://pubmed.ncbi.nlm.nih.gov/28866562/6,  Integrated Workflows for Machine Learning,Paper,Supporting Analysis of Dimensionality Reduction Results with Contrastive Learning,"Dimensionality reduction (DR) is frequently used for analyzing and visualizing high-dimensional data as it provides agood first glance of the data. However, to interpret the DR result for gaining useful insights from the data, it would take additionalanalysis effort such as identifying clusters and understanding their characteristics. While there are many automatic methods (e.g.,density-based clustering methods) to identify clusters, effective methods for understanding a cluster’s characteristics are still lacking. Acluster can be mostly characterized by its distribution of feature values. Reviewing the original feature values is not a straightforwardtask when the number of features is large. To address this challenge, we present a visual analytics method that effectively highlightsthe essential features of a cluster in a DR result. To extract the essential features, we introduce an enhanced usage of contrastiveprincipal component analysis (cPCA). Our method, called ccPCA (contrasting clusters in PCA), can calculate each feature’s relativecontribution to thecontrastbetween one cluster and other clusters. With ccPCA, we have created an interactive system including ascalable visualization of clusters’ feature contributions. We demonstrate the effectiveness of our method and system with case studiesusing several publicly available datasets.",https://arxiv.org/pdf/1905.03911.pdf6,  Workflow Optimization,Paper,Analysis of Flight Variability: a Systematic Approach,"In movement data analysis, there exists a problem of comparing multiple trajectories of moving objects to common ordistinct reference trajectories. We introduce a general conceptual framework for comparative analysis of trajectories and an analyticalprocedure, which consists of (1) finding corresponding points in pairs of trajectories, (2) computation of pairwise difference measures,and (3) interactive visual analysis of the distributions of the differences with respect to space, time, set of moving objects, trajectorystructures, and spatio-temporal context. We propose a combination of visualisation, interaction, and data transformation techniquessupporting the analysis and demonstrate the use of our approach for solving a challenging problem from the aviation domain.",http://geoanalytics.net/and/papers/vast18.pdf6,  Workflow Optimization,Paper,PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories,"Production planning in the manufacturing industry is crucial for fully utilizing factory resources (e.g., machines, raw materials and workers) and reducing costs. With the advent of industry 4.0, plenty of data recording the status of factory resources have been collected and further involved in production planning, which brings an unprecedented opportunity to understand, evaluate and adjust complex production plans through a data-driven approach. However, developing a systematic analytics approach for production planning is challenging due to the large volume of production data, the complex dependency between products, and unexpected changes in the market and the plant. Previous studies only provide summarized results and fail to show details for comparative analysis of production plans. Besides, the rapid adjustment to the plan in the case of an unanticipated incident is also not supported. In this paper, we propose PlanningVis, a visual analytics system to support the exploration and comparison of production plans with three levels of details: a plan overview presenting the overall difference between plans, a product view visualizing various properties of individual products, and a production detail view displaying the product dependency and the daily production details in related factories. By integrating an automatic planning algorithm with interactive visual explorations, PlanningVis can facilitate the efficient optimization of daily production planning as well as support a quick response to unanticipated incidents in manufacturing. Two case studies with real-world data and carefully designed interviews with domain experts demonstrate the effectiveness and usability of PlanningVis.",https://pubmed.ncbi.nlm.nih.gov/31425087/6,  Knowledge-assisted Workflows,Paper,Clustrophile 2: Guided Visual Clustering Analysis,"Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifyingrelevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters aswell as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space isalso challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment toestablish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore andnavigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduceClustrophile 2,a new interactive tool for guided clustering analysis.Clustrophile 2guides users in clustering-based exploratory analysis, adaptsuser feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences betweenclusterings. To this end,Clustrophile 2contributes a novel feature, the Clustering Tour, to help users choose clustering parameters andassess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluateClustrophile 2through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson’s diseasepatients. Results suggest thatClustrophile 2improves the speed and effectiveness of exploratory clustering analysis for both experts andnon-experts.",https://arxiv.org/pdf/1804.03048.pdf6,  Knowledge-assisted Workflows,Paper,KnowledgePearls: Provenance-Based Visualization Retrieval.,"Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guidingusers in future analyses. However, without extensive manual creation of meta information and annotations by the users, search andretrieval of analysis states can become tedious. We presentKnowledgePearls, a solution for efficient retrieval of analysis states thatare structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, wedescribe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requestedanalysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query orinferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses byHans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlyingvisualization framework. We discuss the applicability for visualizations which are based on the declarative grammarVegaand we usea Vega-based implementation ofGapminderas guiding example. We additionally present a biomedical case study to illustrate howKnowledgePearlsfacilitates the exploration process by recalling states from earlier analyses.",http://data.caleydo.org/papers/2018_vast_knowledge-pearls.pdf6,  Knowledge-assisted Workflows,Paper,"Exploring Multivariate Event Sequences using Rules, Aggregations, and Selections","Multivariate event sequences are ubiquitous: travel history, telecommunication conversations, and server logs are someexamples. Besides standard properties such as type and timestamp, events often have other associated multivariate data. Currentexploration and analysis methods either focus on the temporal analysis of a single attribute or the structural analysis of the multivariatedata only. We present an approach where users can explore event sequences at multivariate and sequential level simultaneously byinteractively defining a set of rewrite rules using multivariate regular expressions. Users can store resulting patterns as new types ofevents or attributes to interactively enrich or simplify event sequences for further investigation. In Eventpad we provide a bottom-upglyph-oriented approach for multivariate event sequence analysis by searching, clustering, and aligning them according to newly defineddomain specific properties. We illustrate the effectiveness of our approach with real-world data sets including telecommunication trafficand hospital treatments.",https://www.bramcappers.nl/files/papers/eventPad.pdf